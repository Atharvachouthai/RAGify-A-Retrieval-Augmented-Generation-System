# # import streamlit as st
# # from dotenv import load_dotenv
# # import os
# # import requests
# # from langchain_community.vectorstores import FAISS
# # from langchain.embeddings import HuggingFaceEmbeddings

# # import sys

# # print("Python executable:", sys.executable)
# # print("Python version:", sys.version)

# # # Load environment variables
# # load_dotenv()

# # # Retrieve Hugging Face API key
# # hf_api_key = os.getenv("HF_API_KEY")

# # def query_vectorstore(directory, query, k=5):
# #     """
# #     Query the FAISS vectorstore for relevant documents.
# #     """
# #     embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# #     vectorstore = FAISS.load_local(directory, embedding_model, allow_dangerous_deserialization=True)
# #     return vectorstore.similarity_search(query, k=k)

# # def query_huggingface_api(prompt, model="tiiuae/falcon-7b-instruct"):
# #     """
# #     Query Hugging Face Inference API for LLM responses.
# #     """
# #     url = f"https://api-inference.huggingface.co/models/{model}"
# #     headers = {"Authorization": f"Bearer {hf_api_key}"}
# #     payload = {
# #         "inputs": prompt,
# #         "parameters": {
# #             "max_length": 1024,  # Increase the token limit
# #             "temperature": 0.5
# #         }
# #     }
# # #     payload = {
# # #     "inputs": prompt,
# # #     "parameters": {
# # #         "max_length": 1024,  # Increase the token limit
# # #         "temperature": 0.5
# # #     }
# # # }

# #     response = requests.post(url, headers=headers, json=payload)
# #     if response.status_code == 200:
# #         return response.json()[0]["generated_text"]
# #     else:
# #         raise Exception(f"Error: {response.status_code}, {response.text}")

# # # Streamlit App
# # st.title("RAG System: Financial Insights")
# # st.write("Ask questions about companies and get insights from their 10-K filings.")

# # # Vectorstore Directory
# # vectorstore_dir = "vectorstore"

# # # User Query Input
# # user_query = st.text_input("Enter your question:")

# # if user_query:
# #     # Query the vectorstore
# #     with st.spinner("Retrieving relevant documents..."):
# #         try:
# #             vectorstore_results = query_vectorstore(vectorstore_dir, user_query)

# #             # Combine retrieved documents for LLM input
# #             context = "\n".join([
# #                 f"Document {i+1}:\n{result.page_content}\n(Source: {result.metadata['company']} | {result.metadata['year']} | {result.metadata['section']})\n"
# #                 for i, result in enumerate(vectorstore_results)
# #             ])
# #             prompt = f"Context:\n{context}\n\nQuestion: {user_query}\n\nAnswer:"

# #             # Query Hugging Face API
# #             with st.spinner("Querying LLM..."):
# #                 response = query_huggingface_api(prompt)
            
# #             # Display Results
# #             st.subheader("LLM Response")
# #             st.write(response)

# #             st.subheader("Retrieved Documents")
# #             for idx, result in enumerate(vectorstore_results):
# #                 st.write(f"### Document {idx + 1}")
# #                 st.write(result.page_content)
# #                 st.write(f"**Metadata:** {result.metadata}")
# #                 st.write("---")
# #         except Exception as e:
# #             st.error(f"Error: {e}")


# import streamlit as st
# from dotenv import load_dotenv
# import os
# import requests
# from langchain_community.vectorstores import FAISS
# from langchain.embeddings import HuggingFaceEmbeddings

# # Load environment variables
# load_dotenv()
# hf_api_key = os.getenv("HF_API_KEY")

# def query_vectorstore(directory, query, k=3):
#     embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
#     vectorstore = FAISS.load_local(directory, embedding_model, allow_dangerous_deserialization=True)
#     return vectorstore.similarity_search(query, k=k)

# def query_huggingface_api(prompt, model="tiiuae/falcon-7b-instruct"):
#     url = f"https://api-inference.huggingface.co/models/{model}"
#     headers = {"Authorization": f"Bearer {hf_api_key}"}
#     payload = {
#         "inputs": prompt,
#         "parameters": {"max_length": 1024, "temperature": 0.5}
#     }

#     response = requests.post(url, headers=headers, json=payload)
#     if response.status_code == 200:
#         return response.json()[0]["generated_text"]
#     else:
#         raise Exception(f"Error: {response.status_code}, {response.text}")

# # Streamlit App
# st.title("RAG System: Financial Insights")
# st.write("Ask questions about companies and get insights from their 10-K filings.")

# # User Input
# vectorstore_dir = "vectorstore"
# user_query = st.text_input("Enter your question:")

# if user_query:
#     try:
#         with st.spinner("Retrieving relevant documents..."):
#             vectorstore_results = query_vectorstore(vectorstore_dir, user_query)

#         # Combine retrieved context for the LLM
#         context = "\n".join([
#             f"{result.page_content}"
#             for result in vectorstore_results[:3]  # Limit to top 3 documents
#         ])
#         prompt = f"Context:\n{context}\n\nQuestion: {user_query}\n\nAnswer:"

#         with st.spinner("Querying LLM..."):
#             response = query_huggingface_api(prompt)

#         # Display only the LLM's response
#         st.subheader("LLM Response")
#         st.write(response)

#     except Exception as e:
#         st.error(f"Error: {e}")


# import streamlit as st
# from langchain_community.vectorstores import FAISS
# from langchain.embeddings import HuggingFaceEmbeddings
# import os
# from dotenv import load_dotenv
# import requests

# # Load environment variables
# load_dotenv()
# HF_API_KEY = os.getenv("HF_API_KEY")
# VECTORSTORE_DIR = "vectorstore"

# # Define roles
# ROLES = ["Researcher", "Financial Analyst", "Technical Specialist"]

# def query_vectorstore(directory, query, k=3):
#     embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
#     vectorstore = FAISS.load_local(directory, embedding_model, allow_dangerous_deserialization=True)
#     results = vectorstore.similarity_search(query, k=k)
#     return results

# def query_huggingface_api(prompt, api_key, model="tiiuae/falcon-7b-instruct"):
#     url = f"https://api-inference.huggingface.co/models/{model}"
#     headers = {"Authorization": f"Bearer {api_key}"}
#     payload = {
#         "inputs": prompt,
#         "parameters": {"max_length": 500, "temperature": 0.5}
#     }
#     response = requests.post(url, headers=headers, json=payload)
#     if response.status_code == 200:
#         return response.json()[0]["generated_text"]
#     else:
#         raise Exception(f"Error: {response.status_code}, {response.text}")

# # Streamlit App UI
# st.set_page_config(page_title="RAG System", layout="wide")

# st.markdown("<h1 style='text-align: center;'>✨ Retrieval-Augmented Generation (RAG) System ✨</h1>", unsafe_allow_html=True)
# st.markdown("<p style='text-align: center;'>Ask questions based on a financial dataset enhanced by vectorstore retrieval.</p>", unsafe_allow_html=True)

# with st.sidebar:
#     st.header("System Settings")
#     user_role = st.selectbox("Select your role:", ROLES)
#     user_query = st.text_input("Enter your question:")
#     submit_query = st.button("Submit Query")

# if submit_query:
#     try:
#         st.info("Querying vectorstore...")
#         vectorstore_results = query_vectorstore(VECTORSTORE_DIR, user_query)

#         context = "\n".join([
#             f"Document {i+1}:\n{result.page_content.strip()}\n"
#             for i, result in enumerate(vectorstore_results)
#         ])

#         prompt = f"You are a {user_role}. Based on the following context, provide a concise and complete answer to the question. Do not repeat the context in your response.\n\nContext:\n{context}\n\nQuestion: {user_query}\n\nAnswer:"
        
#         st.info("Querying the LLM...")
#         llm_response = query_huggingface_api(prompt, HF_API_KEY)

#         st.success("Response generated successfully!")

#         # Display Results
#         st.markdown("### Final Response")
#         st.write(llm_response.strip())

#     except Exception as e:
#         st.error(f"An error occurred: {e}")

import os
import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
import requests
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

HF_API_KEY = os.getenv("HF_API_KEY")  # Hugging Face API Key
VECTORSTORE_DIR = "vectorstore"  # Path to your vectorstore directory

# Function to query the FAISS vectorstore
def query_vectorstore(query, k=5):
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = FAISS.load_local(VECTORSTORE_DIR, embedding_model, allow_dangerous_deserialization=True)
    results = vectorstore.similarity_search(query, k=k)
    return results

# Function to query the Hugging Face API
def query_huggingface_api(prompt, model="tiiuae/falcon-7b-instruct"):
    url = f"https://api-inference.huggingface.co/models/{model}"
    headers = {"Authorization": f"Bearer {HF_API_KEY}"}
    payload = {"inputs": prompt, "parameters": {"max_length": 500, "temperature": 0.5}}
    
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        return response.json()[0]["generated_text"]
    else:
        raise Exception(f"Error {response.status_code}: {response.text}")

# Function for the RAG pipeline
def rag_pipeline(role, query):
    # Step 1: Retrieve context from the vectorstore
    context_docs = query_vectorstore(query)
    context_list = [doc.page_content.strip() for doc in context_docs]
    context = " ".join(context_list)  # Combine all documents into a single context
    
    # Step 2: Construct the prompt
    prompt = f"You are a {role}. Provide a concise and complete answer to the question based on the context provided. Do not repeat the context.\n\n"
    prompt += f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
    
    # Step 3: Generate the answer using Hugging Face API
    answer = query_huggingface_api(prompt)
    return answer

# Streamlit app layout
def main():
    st.set_page_config(page_title="RAG System", layout="wide")
    
    # Header Section
    st.markdown(
        """
        <style>
        .main-title {
            font-size: 2.2rem;
            font-weight: bold;
            text-align: center;
            color: #ffffff;
        }
        </style>
        """,
        unsafe_allow_html=True
    )
    st.markdown('<div class="main-title">✨ Retrieval-Augmented Generation (RAG) System ✨</div>', unsafe_allow_html=True)
    st.markdown("Answering your queries with context-aware AI from financial data.")

    # Input Section
    st.sidebar.header("Query Setup")
    role = st.sidebar.selectbox("Select Your Role", ["Financial Analyst", "Researcher", "Data Scientist"])
    user_query = st.sidebar.text_area("Enter Your Query", placeholder="Type your question here...")
    
    if st.sidebar.button("Submit Query"):
        if user_query.strip():
            with st.spinner("Processing your query..."):
                try:
                    # Call the RAG pipeline
                    answer = rag_pipeline(role, user_query)
                    
                    # Status Section
                    st.success("Query processed successfully!")
                    
                    # Output Section
                    st.markdown("### Query Results")
                    
                    # Display only the answer
                    st.subheader("💡 Answer")
                    st.markdown(f"**Question:** {user_query}")
                    st.markdown(f"**Answer:**\n\n{answer}")
                
                except Exception as e:
                    st.error(f"An error occurred: {str(e)}")
        else:
            st.error("Please enter a valid query!")
    
    # Footer Section
    st.markdown("---")
    st.write("🔗 Powered by Hugging Face, LangChain, and Streamlit.")

if __name__ == "__main__":
    main()







